\documentclass[11pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Unsupervised learning - clustering and dimension reduction},
            pdfauthor={Brad Kim},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Unsupervised learning - clustering and dimension reduction}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Brad Kim}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Fall 2018}


\begin{document}
\maketitle

\hypertarget{lab-section}{%
\section{Lab Section}\label{lab-section}}

Download auto data from the github page
\url{https://github.com/ayeaton/BMSC-GA-4439-Fall2018/blob/master/Auto.data.txt}
or the \emph{Statistical Learning} book website here:
\url{http://www-bcf.usc.edu/~gareth/ISL/data.html}

Today, we are going over Hierarchical clustering, K-Means Clustering,
PCA, ICA, and NMF.

\emph{HW- PCA, ICA, K-means clustering, NNFM on iris data}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{setwd}\NormalTok{(}\StringTok{"/Users/MOOSE/Desktop/ML/Assignments/Lab2"}\NormalTok{)}
\NormalTok{Auto_data <-}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"Auto.data.txt"}\NormalTok{, }\DataTypeTok{header=}\NormalTok{T, }\DataTypeTok{stringsAsFactors =}\NormalTok{ F)}
\CommentTok{#remove cars with unknown horsepower and set horsepower to numeric}
\NormalTok{Auto_data <-}\StringTok{ }\NormalTok{Auto_data[}\OperatorTok{-}\KeywordTok{which}\NormalTok{(Auto_data}\OperatorTok{$}\NormalTok{horsepower }\OperatorTok{==}\StringTok{ "?"}\NormalTok{),]}
\NormalTok{Auto_data}\OperatorTok{$}\NormalTok{horsepower <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(Auto_data}\OperatorTok{$}\NormalTok{horsepower)}
\CommentTok{#save car names }
\NormalTok{Auto_data_names <-}\StringTok{ }\NormalTok{Auto_data}\OperatorTok{$}\NormalTok{name}
\CommentTok{#use the numeric values}
\NormalTok{Auto_data_clust <-}\StringTok{ }\NormalTok{Auto_data[,}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{]}
\KeywordTok{dim}\NormalTok{(Auto_data_clust)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 392   8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#392 is too much for a demo, so lets take the first 25}
\NormalTok{Auto_data_clust <-}\StringTok{ }\NormalTok{Auto_data_clust[}\DecValTok{1}\OperatorTok{:}\DecValTok{25}\NormalTok{,]}
\KeywordTok{rownames}\NormalTok{(Auto_data_clust) <-}\StringTok{ }\NormalTok{Auto_data_names[}\DecValTok{1}\OperatorTok{:}\DecValTok{25}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\emph{I'm just gonna take all 150 rows cuz I feel like it. And I'll just
use HPC if my labtop is too slow. Remove the comment for the next 2
lines after dim() if you would like to just use the first 25. Reminder
that the 150 flowers are just named 1-150. The cluster will look messy.}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(iris)}
\NormalTok{iris_subs <-}\StringTok{ }\NormalTok{iris[iris}\OperatorTok{$}\NormalTok{Species,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)]}
\CommentTok{#Why is species saved as factors in R? Need to clean the data.}
\CommentTok{#iris = data.frame(lapply(iris$Species, as.character))}

\NormalTok{iris <-}\StringTok{ }\KeywordTok{factor2character}\NormalTok{(iris)}
\NormalTok{species <-}\StringTok{ }\NormalTok{iris}\OperatorTok{$}\NormalTok{Species}
\CommentTok{#save species as character, not factors}
\NormalTok{iris_clust <-}\StringTok{ }\NormalTok{iris[,}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\KeywordTok{dim}\NormalTok{(iris_clust)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 150   4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#make.names handles the unique row ERROR for rownames}
\KeywordTok{rownames}\NormalTok{(iris_clust) <-}\StringTok{ }\KeywordTok{make.names}\NormalTok{(species, }\DataTypeTok{unique=}\OtherTok{TRUE}\NormalTok{)}


\CommentTok{# I'm just gonna take all 150 rows cuz I feel like it. And I'll just use HPC if my labtop is too slow. Remove the comment for the next 2 lines if you would like to just use the first 25.}

\CommentTok{#iris_clust <- iris_clust[1:25,]}
\CommentTok{#rownames(iris_clust) <- species[1:25]}

\CommentTok{#392 is too much for a demo, so lets take the first 25}
\CommentTok{#Auto_data_clust <- Auto_data_clust[1:25,]}
\CommentTok{#rownames(Auto_data_clust) <- Auto_data_names[1:25]}
\end{Highlighting}
\end{Shaded}

\hypertarget{hierarchical-agglomerative-clustering}{%
\subsection{Hierarchical agglomerative
clustering}\label{hierarchical-agglomerative-clustering}}

Step 1. Assign each item to it's own cluster. We start with 25 clusters,
one for each car.

Step 2. Calculate a similarity matrix between each cluster.

Step 3. Find the pair of clusters closest in similarity.

Step 4. Merge these clusters/recalculate similarity between clusters.
Options are: single linkage (nearest neighbor), complete linkage
(furthest neighbor), average linkage (mean distance between all pairs of
data from the two different clusters), centroid linkage (distance
between the means of all points in the clusters). Now we have 24
clusters.

Step 5. Repeat Step 3 and 4 until there is only one cluster.

\hypertarget{in-practice}{%
\subsubsection{In practice}\label{in-practice}}

Step 1. Each car is a cluster.

Step 2. Create a distance matrix from Auto\_data\_clust.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(}\StringTok{"dist"}\NormalTok{)}
\NormalTok{iris_hierarchical_dist <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{dist}\NormalTok{(iris_clust, }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{))}
\KeywordTok{View}\NormalTok{(iris_hierarchical_dist)}
\end{Highlighting}
\end{Shaded}

Step 3. Find the two cars that are the most similar to each other and
print the names of those two cars

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diag}\NormalTok{(iris_hierarchical_dist) <-}\StringTok{ }\OtherTok{NA}
\KeywordTok{arrayInd}\NormalTok{(}\KeywordTok{which.min}\NormalTok{(iris_hierarchical_dist), }\KeywordTok{dim}\NormalTok{(iris_hierarchical_dist))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]  143  102
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#positions 143 and 102 are the most similar. Lets go back to the names of the cars}
\NormalTok{species[}\DecValTok{143}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "virginica"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{species[}\DecValTok{102}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "virginica"
\end{verbatim}

Step 4. Merge the two clusters together using average linkage.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#replace pos 102 with the average of pos 143 and 102}
\NormalTok{iris_hierarchical_dist[,}\DecValTok{102}\NormalTok{] <-}\StringTok{ }\KeywordTok{apply}\NormalTok{((iris_hierarchical_dist[,}\KeywordTok{c}\NormalTok{(}\DecValTok{143}\NormalTok{,}\DecValTok{102}\NormalTok{)]),}\DecValTok{1}\NormalTok{,mean)}
\NormalTok{iris_hierarchical_dist[}\DecValTok{102}\NormalTok{,] <-}\StringTok{ }\KeywordTok{apply}\NormalTok{((iris_hierarchical_dist[}\KeywordTok{c}\NormalTok{(}\DecValTok{143}\NormalTok{,}\DecValTok{102}\NormalTok{),]),}\DecValTok{2}\NormalTok{,mean)}

\CommentTok{#remove pos 143}
\NormalTok{iris_hierarchical_dist <-}\StringTok{ }\NormalTok{iris_hierarchical_dist[}\OperatorTok{-}\DecValTok{143}\NormalTok{,}\OperatorTok{-}\DecValTok{143}\NormalTok{]}

\CommentTok{#now position 102 represents the cluster containing the vrginicas}
\end{Highlighting}
\end{Shaded}

Step 5. To complete the algorithm, go back to step 3 and iterate through
all of the previous steps until there are no more rows left

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diag}\NormalTok{(iris_hierarchical_dist) <-}\StringTok{ }\OtherTok{NA}
\KeywordTok{arrayInd}\NormalTok{(}\KeywordTok{which.min}\NormalTok{(iris_hierarchical_dist), }\KeywordTok{dim}\NormalTok{(iris_hierarchical_dist))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]   40    8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#postitions 40 and 8 are the most similar}
\NormalTok{species[}\DecValTok{40}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "setosa"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{species[}\DecValTok{8}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "setosa"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#Step 3. Find the two cars that are the most similar to each other and print the names of those two cars}
\KeywordTok{diag}\NormalTok{(iris_hierarchical_dist) <-}\StringTok{ }\OtherTok{NA}
\KeywordTok{arrayInd}\NormalTok{(}\KeywordTok{which.min}\NormalTok{(iris_hierarchical_dist), }\KeywordTok{dim}\NormalTok{(iris_hierarchical_dist))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]   40    8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#replace pos 8 with the average of pos 40 and 8}
\NormalTok{iris_hierarchical_dist[,}\DecValTok{8}\NormalTok{] <-}\StringTok{ }\KeywordTok{apply}\NormalTok{((iris_hierarchical_dist[,}\KeywordTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{,}\DecValTok{8}\NormalTok{)]),}\DecValTok{1}\NormalTok{,mean)}
\NormalTok{iris_hierarchical_dist[}\DecValTok{8}\NormalTok{,] <-}\StringTok{ }\KeywordTok{apply}\NormalTok{((iris_hierarchical_dist[}\KeywordTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{,}\DecValTok{8}\NormalTok{),]),}\DecValTok{2}\NormalTok{,mean)}

\CommentTok{#remove pos 40}
\NormalTok{iris_hierarchical_dist <-}\StringTok{ }\NormalTok{iris_hierarchical_dist[}\OperatorTok{-}\DecValTok{40}\NormalTok{,}\OperatorTok{-}\DecValTok{40}\NormalTok{]}

\CommentTok{#now position 8 represents the cluster containing setosa}
\end{Highlighting}
\end{Shaded}

REPEAT. Don't really need this.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diag}\NormalTok{(iris_hierarchical_dist) <-}\StringTok{ }\OtherTok{NA}
\KeywordTok{arrayInd}\NormalTok{(}\KeywordTok{which.min}\NormalTok{(iris_hierarchical_dist), }\KeywordTok{dim}\NormalTok{(iris_hierarchical_dist))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]   18    1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##postitions 6 and 4 are the most similar. Lets go back to the names of the cars}
\CommentTok{#Auto_data_names[6]}
\CommentTok{#Auto_data_names[4]}
\CommentTok{#}
\CommentTok{##postitions 6 and 4 are the most similar}
\CommentTok{#}
\CommentTok{##replace pos 4 with the average of pos 6 and 4}
\CommentTok{#hierarchical_dist[,4] <- apply((hierarchical_dist[,c(6,4)]),1,mean)}
\CommentTok{#hierarchical_dist[4,] <- apply((hierarchical_dist[c(6,4),]),2,mean)}
\CommentTok{#}
\CommentTok{##remove pos 4}
\CommentTok{#hierarchical_dist <- hierarchical_dist[-6,-6]}
\CommentTok{#}
\CommentTok{##now position 6 represents the cluster containing the ford galaxie 500 and amc rebel sst}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#diag(hierarchical_dist) <- NA}
\CommentTok{#arrayInd(which.min(hierarchical_dist), dim(hierarchical_dist))}

\CommentTok{##postitions 5 and 4 are the most similar. Lets go back to the names of the cars}
\CommentTok{#Auto_data_names[5]}
\CommentTok{#Auto_data_names[4]}
\CommentTok{#}
\CommentTok{##postitions 5 and 4 are the most similar}
\CommentTok{#}
\CommentTok{##replace pos 4 with the average of pos 5 and 4}
\CommentTok{#hierarchical_dist[,4] <- apply((hierarchical_dist[,c(5,4)]),1,mean)}
\CommentTok{#hierarchical_dist[4,] <- apply((hierarchical_dist[c(5,4),]),2,mean)}
\CommentTok{#}
\CommentTok{##remove pos 4}
\CommentTok{#hierarchical_dist <- hierarchical_dist[-5,-5]}
\CommentTok{#}
\CommentTok{##now position 5 represents the cluster containing the ford torino and amc rebel sst}
\CommentTok{#}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-function}{%
\subsubsection{R function}\label{r-function}}

Now that we know how the algorithm works, let's use the R function
hclust. Plot the Dendogram resulting from clustering the
Auto\_data\_clust using average linkage.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_hierarchical_dist <-}\StringTok{ }\KeywordTok{dist}\NormalTok{(iris_clust, }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\NormalTok{hc_iris <-}\StringTok{ }\KeywordTok{hclust}\NormalTok{(iris_hierarchical_dist, }\DataTypeTok{method=}\StringTok{"average"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(hc_iris)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-10-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load code of A2R function}
\KeywordTok{source}\NormalTok{(}\StringTok{"http://addictedtor.free.fr/packages/A2R/lastVersion/R/code.R"}\NormalTok{)}
\CommentTok{# colored dendrogram}
\NormalTok{op =}\StringTok{ }\KeywordTok{par}\NormalTok{(}\DataTypeTok{bg =} \StringTok{"#EFEFEF"}\NormalTok{)}
\KeywordTok{A2Rplot}\NormalTok{(hc_iris, }\DataTypeTok{k =} \DecValTok{3}\NormalTok{, }\DataTypeTok{boxes =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col.up =} \StringTok{"gray50"}\NormalTok{, }\DataTypeTok{col.down =} \KeywordTok{c}\NormalTok{(}\StringTok{"#FF6B6B"}\NormalTok{, }
    \StringTok{"#4ECDC4"}\NormalTok{, }\StringTok{"#556270"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{A2Rplot}\NormalTok{(hc_iris, }\DataTypeTok{k =} \DecValTok{2}\NormalTok{, }\DataTypeTok{boxes =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{col.up =} \StringTok{"gray50"}\NormalTok{, }\DataTypeTok{col.down =} \KeywordTok{c}\NormalTok{(}\StringTok{"#FF6B6B"}\NormalTok{, }\StringTok{"#4ECDC4"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-11-2.pdf}

\newpage

\#\#K-Means Clustering Step 1. Choose the N number of clusters. 2 cuz
the plots say so.

Step 2. Find the N items that are furthest apart and set them as cluster
centroids.

Step 3. Assign one item in the dataset to the closest of the N cluster
centroids.

Step 4. Recalculate the cluster centroid.

Step 5. Repeat Steps 3 and 4 until all items are in a cluster.

Step 6. Go through each item and reassess whether the item belongs in
the current cluster or in a different cluster based on distance to
cluster centroids. Every time an item is reassigned to a different
cluster, the centroids must be recalculated.

Step 7. When every item belongs firmly to a cluster, or the iterations
of Step 6 are endless, the algorithm is complete.

\hypertarget{in-practice-1}{%
\subsubsection{In practice}\label{in-practice-1}}

Step 1. We are going to cluster the 150 flowers into two groups.

Step 2a. Find the two flowers furthest from each other.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(iris, }\KeywordTok{aes}\NormalTok{(Petal.Length, Petal.Width, }\DataTypeTok{color =}\NormalTok{ Species)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/Plots-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(iris, }\KeywordTok{aes}\NormalTok{(Sepal.Length, Sepal.Width, }\DataTypeTok{color =}\NormalTok{ Species)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/Plots-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(iris, }\KeywordTok{aes}\NormalTok{(Petal.Length, Sepal.Width, }\DataTypeTok{color =}\NormalTok{ Species)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/Plots-3.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(iris, }\KeywordTok{aes}\NormalTok{(Sepal.Length, Petal.Width, }\DataTypeTok{color =}\NormalTok{ Species)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/Plots-4.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_kmeans_dist <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{dist}\NormalTok{(iris_clust, }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{))}
\KeywordTok{diag}\NormalTok{(iris_kmeans_dist) <-}\StringTok{ }\OtherTok{NA}
\KeywordTok{arrayInd}\NormalTok{(}\KeywordTok{which.max}\NormalTok{(iris_kmeans_dist), }\KeywordTok{dim}\NormalTok{(iris_kmeans_dist))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,]  119   14
\end{verbatim}

Step 2b. Create data frames to hold each cluster, cluster names, and
centroids.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_cluster_one <-}\StringTok{ }\NormalTok{iris_clust[}\DecValTok{119}\NormalTok{,]}
\NormalTok{iris_cluster_one_names <-}\StringTok{ }\NormalTok{species[}\DecValTok{119}\NormalTok{]}
\NormalTok{iris_cluster_one_centroid <-}\StringTok{ }\NormalTok{iris_cluster_one}

\NormalTok{iris_cluster_two <-}\StringTok{ }\NormalTok{iris_clust[}\DecValTok{14}\NormalTok{,]}
\NormalTok{iris_cluster_two_names <-}\StringTok{ }\NormalTok{species[}\DecValTok{119}\NormalTok{]}
\NormalTok{iris_cluster_two_centroid <-}\StringTok{ }\NormalTok{iris_cluster_two}
\end{Highlighting}
\end{Shaded}

Step 3a. Sequentially put cars in either cluster one or cluster two.
lets start with car 1. Is car 1 closer to cluster one or cluster two?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#distance to cluster 1}
\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(iris_cluster_one_centroid,iris_clust[}\DecValTok{1}\NormalTok{,]), }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        virginica.18
## setosa     6.498461
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#distance to cluster 2}
\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(iris_cluster_two_centroid,iris_clust[}\DecValTok{1}\NormalTok{,]), }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        setosa.13
## setosa 0.9949874
\end{verbatim}

Step 3b. Add car 1 to cluster two and adjust the centroid value for
cluster 2. The new centroid value is a mean of the values of cars in
that cluster.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_cluster_two <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(iris_cluster_two, iris_clust[}\DecValTok{1}\NormalTok{,])}
\NormalTok{iris_cluster_two_names <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(iris_cluster_two_names, species[}\DecValTok{1}\NormalTok{])}
\NormalTok{iris_cluster_two_centroid <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(iris_cluster_two,}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

Step 3d. Do for all flowers

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(iris_clust))\{}
  \ControlFlowTok{if}\NormalTok{(i }\OperatorTok{==}\StringTok{ }\DecValTok{102} \OperatorTok{|}\StringTok{ }\NormalTok{i }\OperatorTok{==}\StringTok{ }\DecValTok{143}\NormalTok{)\{}
    \ControlFlowTok{next}
\NormalTok{  \}}
  \ControlFlowTok{if}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(iris_cluster_two_centroid,iris_clust[i,]), }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{) }\OperatorTok{<}\StringTok{ }\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(iris_cluster_one_centroid,iris_clust[i,]), }\DataTypeTok{method =} \StringTok{"euclidean"}\NormalTok{) ) \{}
\NormalTok{    iris_cluster_two <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(iris_cluster_two, iris_clust[i,])}
\NormalTok{    iris_cluster_two_names <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(iris_cluster_two_names, species[i])}
\NormalTok{    iris_cluster_two_centroid <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(iris_cluster_two,}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(x)))}
\NormalTok{  \}}\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{    iris_cluster_one <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(iris_cluster_one, iris_clust[i,])}
\NormalTok{    iris_cluster_one_names <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(iris_cluster_one_names, species[i])}
\NormalTok{    iris_cluster_one_centroid <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(iris_cluster_one,}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{as.numeric}\NormalTok{(x)))}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Step 4. Adjust the clusters by comparing the distance of each car to the
centroid of its current cluster versus the distance to the centroid of
the other cluster. Does it still belong in the current cluster? lets
start with car 1. Does it belong in cluster two?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(iris_cluster_one_centroid,iris_clust[}\DecValTok{1}\NormalTok{,])))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               1
## setosa 5.754075
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(}\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(iris_cluster_two_centroid,iris_clust[}\DecValTok{1}\NormalTok{,])))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                1
## setosa 0.2540102
\end{verbatim}

Alright, so car 1 does belong in cluster two. To complete the algorithm,
iterate over the cars until none of the cars switch clusters.

\hypertarget{r-function-1}{%
\subsubsection{R function}\label{r-function-1}}

Now we know how the algorithm works, lets use the R function kmeans.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(kmeans)}
\NormalTok{iris_kmeans_put <-}\StringTok{ }\KeywordTok{kmeans}\NormalTok{(iris_clust,}\DecValTok{2}\NormalTok{)}
\KeywordTok{str}\NormalTok{(iris_kmeans_put)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 9
##  $ cluster     : Named int [1:150] 1 1 1 1 1 1 1 1 1 1 ...
##   ..- attr(*, "names")= chr [1:150] "setosa" "setosa.1" "setosa.2" "setosa.3" ...
##  $ centers     : num [1:2, 1:4] 5.01 6.3 3.37 2.89 1.56 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:2] "1" "2"
##   .. ..$ : chr [1:4] "Sepal.Length" "Sepal.Width" "Petal.Length" "Petal.Width"
##  $ totss       : num 681
##  $ withinss    : num [1:2] 28.6 123.8
##  $ tot.withinss: num 152
##  $ betweenss   : num 529
##  $ size        : int [1:2] 53 97
##  $ iter        : int 1
##  $ ifault      : int 0
##  - attr(*, "class")= chr "kmeans"
\end{verbatim}

\newpage

\hypertarget{principal-components-analysis-pca}{%
\subsection{Principal Components Analysis
(PCA)}\label{principal-components-analysis-pca}}

Principal Components Analysis is a linear dimensionality reduction
algorithm. If you want to learn more about linear algebra, I suggest the
MIT Open Courseware class here :
\url{https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/}
There are two ways of doing PCA, Single Value Decomposition (SVD), and
the method we will use today, using the covariance matrix of the data.

Step 1. Center data by subtracting the mean.

Step 2. Calculate covariance matrix of data.

Step 3. Perform Eigendecomposition of the covariance matrix.
i.e.~represent the matrix in terms of it's eigenvalues and eigen vectors

Step 4. Multiply the eigen vectors by the original data to express the
data in terms of the eigen vectors.

Step 1. Center the data by subtracting the mean of the each column from
the values in that column

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#we need a square matrix}
\NormalTok{iris_clust_pca <-}\StringTok{ }\KeywordTok{data.matrix}\NormalTok{(iris_clust[}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{,])}

\NormalTok{Center_iris <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(iris_clust_pca, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) x }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x))}
\end{Highlighting}
\end{Shaded}

Step 2. Calculate covariance matrix of the Auto data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Covariance_iris <-}\StringTok{ }\KeywordTok{cov}\NormalTok{(Center_iris)}
\NormalTok{Covariance_iris}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            setosa setosa.1 setosa.2 setosa.3
## setosa   4.750000 4.421667 4.353333 4.160000
## setosa.1 4.421667 4.149167 4.055000 3.885000
## setosa.2 4.353333 4.055000 3.990000 3.813333
## setosa.3 4.160000 3.885000 3.813333 3.656667
\end{verbatim}

Step 3. Calculate eigen values and vectors

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Eigen_value_iris <-}\StringTok{ }\KeywordTok{eigen}\NormalTok{(Covariance_iris)}\OperatorTok{$}\NormalTok{value}

\CommentTok{#columns are the eigen vectors}
\NormalTok{Eigen_vector_iris <-}\StringTok{ }\KeywordTok{eigen}\NormalTok{(Covariance_iris)}\OperatorTok{$}\NormalTok{vector}
\end{Highlighting}
\end{Shaded}

Step 4. Multiply the eigen vector matrix by the original data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PC <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{data.matrix}\NormalTok{(Center_iris) }\OperatorTok{%*%}\StringTok{ }\NormalTok{Eigen_vector_iris)}

\KeywordTok{ggplot}\NormalTok{(PC, }\KeywordTok{aes}\NormalTok{(PC[,}\DecValTok{1}\NormalTok{], PC[,}\DecValTok{2}\NormalTok{])) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(PC[,}\DecValTok{1}\NormalTok{], PC[,}\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-21-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#+ geom_text(aes(label=Auto_data_names[1:8]), nudge_x = -2.5, nudge_y = 400)}
\end{Highlighting}
\end{Shaded}

Step 5. Find out which principal components explain the variance in the
data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#for each component, take the cumulative sum of eigen values up to that point and and divide by the total sum of eigen values}
\KeywordTok{round}\NormalTok{(}\KeywordTok{cumsum}\NormalTok{(Eigen_value_iris)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(Eigen_value_iris) }\OperatorTok{*}\StringTok{ }\DecValTok{100}\NormalTok{, }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  99.83  99.96 100.00 100.00
\end{verbatim}

Principal component 1 and 2 explain 99.99 percent of the variance.
Principal component 1,2, and 3 together explain 100\% of the variance in
the data.

\hypertarget{r-function-2}{%
\subsubsection{R function}\label{r-function-2}}

Now that we know how PCA works, lets use the R funtion prcomp.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{help}\NormalTok{(}\StringTok{"prcomp"}\NormalTok{)}
\KeywordTok{autoplot}\NormalTok{(}\KeywordTok{prcomp}\NormalTok{(}\KeywordTok{t}\NormalTok{(iris_clust_pca)))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-23-1.pdf}
\emph{Honestly, I'm not convinced that this PCA plot is correct\ldots{},
but maybe some feedback would help. I'm guessing that because 99.83\%
and 0.13\% add up to 99.96\%, component 3 is still explains 0.04\%?}
\newpage

\hypertarget{independent-component-analysis-ica}{%
\subsection{Independent Component Analysis
(ICA)}\label{independent-component-analysis-ica}}

ICA is an algorithm that finds components that are independent, or
subcomponents of the data.

Step 1. Whiten the data by projecting the data onto the eigen vectors
(PCA).

Step 2. Solve the X=AS equation by maximizing non-gaussianty in the
variables(components) in S.

This results in a matrix S with components that are independent from
each other.

We will use the fastICA algorithm.

First we will go backwards. Create a matrix S with the independent
components

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#create two signals}
\NormalTok{S <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{cos}\NormalTok{((}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{)}\OperatorTok{/}\DecValTok{10}\NormalTok{), ((}\DecValTok{500}\OperatorTok{:}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{1000}\NormalTok{))}

\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfcol =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(S[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(S[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-24-1.pdf}

Create a mixing matrix A

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.423}\NormalTok{, }\FloatTok{0.857}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Mix S using A

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\NormalTok{S }\OperatorTok{%*%}\StringTok{ }\NormalTok{A}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfcol =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(X[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(X[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{type=}\StringTok{"l"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-26-1.pdf}

Unmix using fastICA

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfcol =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{, a}\OperatorTok{$}\NormalTok{S[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"S'1"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{""}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{500}\NormalTok{, a}\OperatorTok{$}\NormalTok{S[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{type =} \StringTok{"l"}\NormalTok{, }\DataTypeTok{xlab =} \StringTok{"S'2"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-28-1.pdf}

\hypertarget{ica-on-the-iris}{%
\subsubsection{ICA on the iris}\label{ica-on-the-iris}}

plot the independent components as a heatmap

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{heatmap}\NormalTok{(a}\OperatorTok{$}\NormalTok{S)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-30-1.pdf}

\newpage

\hypertarget{non-negative-matrix-factorization}{%
\subsection{Non-negative Matrix
Factorization}\label{non-negative-matrix-factorization}}

NMF is an algorithm that factorizes the given matrix into two matrices.
All three matrices must have no negative values.

\[V_{mxn}=W_{mxp}H_{pxn}\] Where p is specified to the algorithm. p can
be thought of as the number of features to search for. The column vector
W can be thought of as the features, and the vector H van be thought of
as the weights for these features.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_clust2 <-}\StringTok{ }\NormalTok{iris[,}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]}

\NormalTok{nmf_out <-}\StringTok{ }\KeywordTok{nmf}\NormalTok{(iris_clust2, }\DecValTok{4}\NormalTok{, }\KeywordTok{set.seed}\NormalTok{(}\DecValTok{304543}\NormalTok{), }\DataTypeTok{nrun=} \DecValTok{100}\NormalTok{)}
\NormalTok{W <-}\StringTok{ }\NormalTok{nmf_out}\OperatorTok{@}\NormalTok{fit}\OperatorTok{@}\NormalTok{W}
\NormalTok{H <-}\StringTok{ }\NormalTok{nmf_out}\OperatorTok{@}\NormalTok{fit}\OperatorTok{@}\NormalTok{H}

\CommentTok{#W}
\KeywordTok{basismap}\NormalTok{(nmf_out)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-31-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#H}
\KeywordTok{coefmap}\NormalTok{(nmf_out)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Clustering_student_Moosun_Brad_Kim_iris_files/figure-latex/unnamed-chunk-31-2.pdf}
\newpage

\#\#\#Homework

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(iris)}
\NormalTok{iris_subs <-}\StringTok{ }\NormalTok{iris[,}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{)]}
\NormalTok{species <-}\StringTok{ }\NormalTok{iris[,}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Run PCA, ICA, and NMF on the iris dataset.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Explain your inputs and outputs from each algorithm. For instance, in
  the input for the NMF example above, out inputs were a 25 x 8 matrix,
  and a rank of 4. The output was a matrix W with dimensions 25x4, and a
  matrix H with dimensions 4x8. We plotted the basis matrix (W), where
  each columns corresponds to a feature\ldots{} etc.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Use the silhouette function in the cluster package to find the optimal
  number of clusters for kmeans for the iris dataset. Then cluster using
  hierarchical clustering and kmeans clustering. Does the data cluster
  by species?
\end{enumerate}

\hypertarget{optional-material}{%
\section{Optional material}\label{optional-material}}

On PCA:

Eigen Vectors and Eigen Values
\url{http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/}
Linear Algebra by Prof.~Gilbert Strang
\url{https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/}
\url{http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf}
\url{https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues}

On ICA:

Independent Component Analysis: Algorithms and Applications
\url{https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf} Tutorial
on ICA taken from
\url{http://rstudio-pubs-static.s3.amazonaws.com/93614_be30df613b2a4707b3e5a1a62f631d19.html}


\end{document}
